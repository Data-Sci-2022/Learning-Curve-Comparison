---
title: "Final Project"
author: "Soobin Choi"
date: "2022-10-01"
output: 
  github_document: 
    toc: TRUE
---

# Developmental process of L2 - English and Korean

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="")
library(tidyverse)
library(tidytext)
library(repurrrsive)
```


## Korean Learners' Corpus (KLC)

### Data Import


```{r}
KLC <- read_tsv(file = "https://github.com/jungyeul/korean-learner-corpus/raw/main/data/kyunghee_v2.tsv", locale(encoding = "UTF-8")) 
```


### Data Manipulation


```{r}
# change column names with clearer description
KLC <- KLC %>% 
  rename(ID = X1,
         Nationality = X2,
         Gender = X3,
         Topic = X4,
         Text = X5,
         Morphemes = X6,
         Level = X7,
         Score = X8)

# remove some values with error
KLC_clean <- KLC %>% 
  filter(Level %in% c("A1", "A2", "B1", "B2", "C1", "C2"))

```



```{r}
# filter the data with nationality.
KLC_eng <- KLC_clean %>% 
  filter(Nationality %in% c("미국", "영국", "호주", "필리핀", "싱가포르", "인도", "르완다"))

KLC_eng %>% 
  select(ID, Nationality, Text) %>% 
  unnest_tokens(word, Text) %>% 
  group_by(ID)

KLC_eng %>% 
  select(ID,Nationality, Text)

KLC_token <- KLC_eng %>% 
  unnest_tokens(word, Text) %>% 
  count(ID, name = "token_num")

# 모르겠으면 F1을 눌러보자ㅎㅎ

KLC_inner <- KLC_clean %>% 
  filter(Nationality %in% c("미국", "영국", "호주"))
```

When filtering nationality of participants, I was not sure what sort of criterium I should apply. I know a huge part of the population in Thailand speaks English, then should I include them too?

So, I ended up sorting out the countries whose official language includes English.
It would have been easier for me to sort out if there is a column about their first language, not their nationality.

### Tokenization

```{r}
KLC_tok <- KLC_eng %>% 
  select(ID, Nationality, Text) %>% 
  unnest_tokens(word, Text) %>% 
  group_by(ID)

KLC_eng %>% 
  select(ID, Nationality, Text) %>% 
  unnest_tokens(word, Text) %>% 
  group_by(ID)
```

#### 1. Lexical Diversity

how to calculate lexical diversity: how many different words are used in each person's essay?; the number of _word type_ divided by the number of _tokens_

```{r}
KLC_eng$Morphemes %>%
  map(~ str_split(., " (\\+)?"))

KLC_clean <- KLC_eng %>%
  select(ID, Morphemes) %>% 
  map(~ str_split(., " (\\+)?")) %>% 
  as_data_frame() %>% 
  rename(num_token = Morphemes) %>% 
  unnest(ID) %>% 
  left_join(KLC_eng, .,  by="ID")

nrow(unique(unnest(KLC_clean[1,9])))
unique(unnest(KLC_clean[2,9]))
unique(unnest(KLC_clean[3,9]))

KLC_clean %>% 
  group_by(ID) %>% 
  select(num_token) %>% 
  unnest() %>% 
  unique() 




```

```{r}
x <- rerun(2, sample(4))
x
unlist(x)
x %>% 
  as_tibble(.name_repair = "universal")
x %>% flatten() %>% 
  as_tibble(.name_repair = "universal")
x %>% flatten_int()

# You can use flatten in conjunction with map
x %>% map(1L) %>% flatten_int()
# But it's more efficient to use the typed map instead.
x %>% map_int(1L)
```


#### 2. Syntactic Complexity

how to calculate syntactic complexity: how many words are used in one sentence and what are the mean of each essay?

```{r}
KLC_small <- head(KLC_eng, 5)
KLC_small
```


```{r}
a <- KLC_eng %>%
  select(ID, Morphemes) %>% 
  map(~ str_split(., " (\\+)?")) %>% 
  as_data_frame() %>% 
  rename(num_token = Morphemes) %>% 
  unnest(ID)  
  count(num_token)
a

KLC_eng %>%
  select(ID, Morphemes) %>% 
  map(~ str_split(., " (\\+)?")) %>% 
  as_data_frame() %>% 
  rename(num_token = Morphemes) %>% 
  unnest(ID)
  
count(unique(unnest(a$num_token)))

KLC_eng %>%
  select(ID, Morphemes) %>% 
  map(~ str_split(., " (\\+)?")) %>% 
  as_data_frame() %>% 
  map()

```



## PELIC


### Data Import


```{r}
PELIC_ans <- read_csv("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/answer.csv")

PELIC_crs <- read_csv("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/course.csv")

PELIC_id <- read_csv("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/student_information.csv")

PELIC_scr <- read_csv("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/test_scores.csv")
```

### Data manipulation

** sorting out columns needed
```{r}

PELIC_scr1 <- PELIC_scr %>% 
  select(anon_id, MTELP_Conv_Score, Writing_Sample)

PELIC_crs1 <- PELIC_crs %>% 
  select(course_id, level_id)

PELIC_ans1 <- PELIC_ans %>%
  select(anon_id, course_id, text_len, text, tokens, tok_lem_POS)
  
PELIC_id1 <- PELIC_id %>% 
  select(anon_id, native_language)

```

* course_id: a unique identifier for each course - a 1-4 digit integer, e.g. 987

* level_id:	a code to identify in which of the four levels the text was produced (5 the highest)



** Joining all the columns in one dataframe
```{r}
PELIC1 <- left_join(PELIC_ans1, PELIC_crs1, by = "course_id") %>%  
  relocate(c(level_id), .after = course_id)
PELIC2 <- left_join(PELIC1, PELIC_id1,by = "anon_id")
PELIC3 <- left_join(PELIC2, PELIC_scr1, by = "anon_id")

PELIC_clean <- PELIC3 %>% 
  filter(native_language == "Korean", text_len > 10, level_id == 5) %>% 
  relocate(c(text, tokens),.after = Writing_Sample)

head(PELIC_clean)

```

#### 1. Lexical Diversity

how to calculate lexical diversity: how many different words are used in each person's essay?; the number of _word type_ divided by the number of _tokens_

```{r}
head(PELIC_clean, 10) %>% 
  select(anon_id, tok_lem_POS) %>% 
  mutate(tok_lem_POS = tok_lem_POS %>% 
           str_remove_all("\\[") %>% 
           str_remove_all("\\]") %>% 
           str_split("(\\))+,")) %>% # I can't figure out how to split the string in the correct way. I wanna leave ')' in each value.
  unnest(tok_lem_POS)
  
           


  mutate(lemma = map(tok_lem_POS,2),
         POS = map(tok_lem_POS,3)) %>% 
  unnest(lemma, POS)



```


#### 2. Syntactic Complexity

how to calculate syntactic complexity: how many words are used in one sentence and what are the mean of each essay?

```{r}
PELIC_clean %>% 
  select
```






##### codes needed for my project from Dan (Thanks!!!)


```{r}
PELIC_ans %>% 
  mutate(tokens = tokens %>% 
           str_remove_all("\\['|'\\]|\\s") %>% 
           str_split("','")) %>% 
  unnest(tokens)
```

```{r}
tibble(col1 = list(c('I', 'I', 'PRP'),
                   c('met', 'meet', 'VBD'),
                   c('my', 'my', 'PRP$'))) %>%
  mutate(Word = map(col1, 1),
         Lemma = map(col1, 2),
         POSTag = map(col1, 3)) %>% 
  unnest(Word:POSTag)
```

```{r}
KLC_clean$Morphemes[1] %>% 
  str_split("\\s(?!\\+)") %>% 
  flatten_chr() %>%
  str_split(" \\+")
```




```{r}
PELIC_ans1 %>% 
  unnest_tokens(word, text) %>% 
  left_join(PELIC_ans %>% select(), by="")

# 이렇게하면 unnest하느라 없어졌던 열들을 다시 갖다 붙일 수 있음!

# %>% mutate(number = str_extract(colname, "regex"))
# 특정 열에서 특정 표현만 추출.
```




